{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e52f643-722a-4c96-8e5c-bb4381a4c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f90b0-69eb-48e4-9607-a2e9395805e1",
   "metadata": {},
   "source": [
    "## Load original train datasets that active learner was applied to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538ba705-5d85-4f6f-bd9e-f8339d4f4d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for19_pt\n",
      "ous19_fr\n",
      "bas19_es\n",
      "ous19_ar\n",
      "san20_it\n"
     ]
    }
   ],
   "source": [
    "df_dict = dict()\n",
    "\n",
    "PATH = \"../0_data/main/1_clean\"\n",
    "    \n",
    "for dataset in os.listdir(PATH):\n",
    "    for f in glob.glob(f\"{PATH}/{dataset}/train*.csv\"):\n",
    "        if \"dyn21\" not in f and \"ipynb\" not in f:\n",
    "            print(dataset[:8])\n",
    "            df_dict[dataset[:8]] = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11428001-a47c-464e-8ece-948d5f2f8b44",
   "metadata": {},
   "source": [
    "## Merge train datasets with prediction logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24449033-d869-4ce5-b87e-69b80d1ed0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ous19_ar\n",
      "for19_pt\n",
      "san20_it\n",
      "ous19_fr\n",
      "bas19_es\n"
     ]
    }
   ],
   "source": [
    "PATH = \"../0_data/main/2_active_learning\"\n",
    "AL_MODEL = \"xlmt_dyn21_en_20000_rs1\"\n",
    "\n",
    "for dataset in os.listdir(PATH):\n",
    "    print(dataset[:8])\n",
    "    df_dict[dataset[:8]] = df_dict[dataset[:8]].merge(pd.read_csv(f\"{PATH}/{dataset}/{AL_MODEL}.csv\")[[\"prediction\", \"logits\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f6eaf-ab02-4ebb-bf42-94500c3cf5d2",
   "metadata": {},
   "source": [
    "## Create columns for selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51b66c15-74aa-4088-8504-b1dffee1a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "for dataset in df_dict:\n",
    "    df_dict[dataset][\"softmax_scores\"] = df_dict[dataset].logits.apply(lambda x: softmax(literal_eval(x)))\n",
    "    df_dict[dataset][\"softmax_diff\"] = df_dict[dataset].softmax_scores.apply(lambda x: abs(x[0]-x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8031d1-a793-4027-9fea-c5c07041b9ae",
   "metadata": {},
   "source": [
    "## Select based on difference in softmax scores across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0da2b8e3-030e-4cb7-8e47-daed076dcc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR19_PT\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "OUS19_FR\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "BAS19_ES\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "OUS19_AR\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n",
      "SAN20_IT\n",
      "  saving n = 10 training set (selected by active learning)\n",
      "  saving n = 20 training set (selected by active learning)\n",
      "  saving n = 30 training set (selected by active learning)\n",
      "  saving n = 40 training set (selected by active learning)\n",
      "  saving n = 50 training set (selected by active learning)\n",
      "  saving n = 100 training set (selected by active learning)\n",
      "  saving n = 200 training set (selected by active learning)\n",
      "  saving n = 300 training set (selected by active learning)\n",
      "  saving n = 400 training set (selected by active learning)\n",
      "  saving n = 500 training set (selected by active learning)\n",
      "  saving n = 1000 training set (selected by active learning)\n",
      "  saving n = 2000 training set (selected by active learning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create differently-sized train portions from rest of data\n",
    "\n",
    "N_RANGE = [10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000, 2000]\n",
    "\n",
    "for dataset in df_dict:\n",
    "    print(dataset.upper())\n",
    "    for n in N_RANGE:\n",
    "        print(f\"  saving n = {n} training set (selected by active learning)\")\n",
    "        export_dict = df_dict[dataset].sort_values(\"softmax_diff\")[[\"text\", \"label\"]][:n]\n",
    "        for file in glob.glob(f\"../0_data/main/2_active_learning/{dataset}*\"):\n",
    "            export_dict.to_csv(f\"{file}/train/train_{n}_al.csv\",index=False)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "428773b4-7371-4cc9-9253-60e254287b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>(-0.88551676, 0.7890035)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>(-0.9222206, 0.8876971)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(-1.284241, 1.1302452)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(-0.16130793, 0.031392984)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.9738463, -1.1099527)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>5595</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(-0.50333995, 0.3105694)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5596</th>\n",
       "      <td>5596</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>(-2.8639174, 2.5761027)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5597</th>\n",
       "      <td>5597</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(1.1118305, -1.1924204)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5598</th>\n",
       "      <td>5598</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(-1.2798216, 1.0581983)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5599</th>\n",
       "      <td>5599</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(-2.1546922, 1.8866277)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  label  prediction                      logits\n",
       "0         0      0           1    (-0.88551676, 0.7890035)\n",
       "1         1      0           1     (-0.9222206, 0.8876971)\n",
       "2         2      1           1      (-1.284241, 1.1302452)\n",
       "3         3      1           1  (-0.16130793, 0.031392984)\n",
       "4         4      0           0     (0.9738463, -1.1099527)\n",
       "...     ...    ...         ...                         ...\n",
       "5595   5595      1           1    (-0.50333995, 0.3105694)\n",
       "5596   5596      0           1     (-2.8639174, 2.5761027)\n",
       "5597   5597      1           0     (1.1118305, -1.1924204)\n",
       "5598   5598      1           1     (-1.2798216, 1.0581983)\n",
       "5599   5599      1           1     (-2.1546922, 1.8866277)\n",
       "\n",
       "[5600 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[\"san20_it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54ffca8e-1dd1-4702-bd99-19fa2fa83e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15782244, 0.84217756])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(literal_eval(df_dict[\"san20_it\"].logits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd3f3eb9-056f-4979-a0c5-865071005703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('(-0.88551676, 0.7890035)', dtype='<U24')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(df_dict[\"san20_it\"].logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "787232a6-8aa9-45fd-b38b-cf7cfe79798c",
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'maximum' did not contain a loop with signature matching types (dtype('<U24'), dtype('<U24')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     e_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(x \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e_x \u001b[38;5;241m/\u001b[39m e_x\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m----> 6\u001b[0m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msan20_it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(x):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute softmax values for each sets of scores in x.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     e_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(x \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e_x \u001b[38;5;241m/\u001b[39m e_x\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Uni/PhD - Oxford/0 - Thesis/0_Articles/6_Low Resource Hate Speech Detection/low-resource-hate-speech-detection/env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2791\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2677\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Uni/PhD - Oxford/0 - Thesis/0_Articles/6_Low Resource Hate Speech Detection/low-resource-hate-speech-detection/env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'maximum' did not contain a loop with signature matching types (dtype('<U24'), dtype('<U24')) -> None"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "softmax(np.asarray(df_dict[\"san20_it\"].logits[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08e6eee8-0f28-4076-8863-429d98930519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes we have model predictions (pred_label) and uncertainty (pred_score) for each entry\n",
    "# could also do §cross-entropy for uncertainty\n",
    "# we only use the train set\n",
    "# the test set remains completely held-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "362abd89-da67-4e00-8b31-d1b08ac87619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy column for uncertainty while waiting for real results\n",
    "for dataset in df_dict:\n",
    "    df_dict[dataset][\"pred_score\"] = df_dict[dataset].label.apply(lambda x: random.uniform(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46d968c7-d5aa-4f3b-a248-48dcd3e6931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ousidhoum2019_french\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "ousidhoum2019_arabic\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "fortuna2019_portuguese\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n",
      "sanguinetti2020_italian\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "  saving n = 5000 training set\n",
      "\n",
      "basile2019_spanish\n",
      "  saving n = 10 training set\n",
      "  saving n = 20 training set\n",
      "  saving n = 50 training set\n",
      "  saving n = 100 training set\n",
      "  saving n = 200 training set\n",
      "  saving n = 500 training set\n",
      "  saving n = 1000 training set\n",
      "  saving n = 2000 training set\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select top-n entries based on active learning\n",
    "# this is deterministic, so no need for multiple random seeds\n",
    "\n",
    "N_RANGE = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000]\n",
    "\n",
    "for dataset in df_dict:\n",
    "    print(dataset)\n",
    "    df_dict[dataset].sort_values(by=\"pred_score\", inplace=True)\n",
    "    for n in N_RANGE:\n",
    "        if n<len(df_dict[dataset]):\n",
    "            print(f\"  saving n = {n} training set\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214e29b-d2a6-4eb9-9754-121694c16280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
